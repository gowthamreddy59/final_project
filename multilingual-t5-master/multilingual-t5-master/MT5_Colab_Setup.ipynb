{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80878ab5",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e5f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q t5==0.9.3 seqio tensorflow-datasets tensorflow-text gin-config sentencepiece nltk pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fb5199",
   "metadata": {},
   "source": [
    "## 2. Clone Project Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the multilingual-t5 repository\n",
    "!git clone https://github.com/google-research/multilingual-t5.git\n",
    "%cd multilingual-t5\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f29ce9",
   "metadata": {},
   "source": [
    "## 3. Verify Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e65e21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if multilingual_t5 can be imported\n",
    "import sys\n",
    "sys.path.insert(0, '/content/multilingual-t5')\n",
    "\n",
    "try:\n",
    "    import multilingual_t5\n",
    "    print(\"âœ“ multilingual_t5 imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"âœ— Error importing multilingual_t5: {e}\")\n",
    "\n",
    "# Check key dependencies\n",
    "import tensorflow as tf\n",
    "import seqio\n",
    "import t5\n",
    "print(f\"âœ“ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"âœ“ SeqIO version: {seqio.__version__}\")\n",
    "print(f\"âœ“ T5 version: {t5.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89c6d8",
   "metadata": {},
   "source": [
    "## 4. Explore Project Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d512328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# List all Python files\n",
    "print(\"Python files in multilingual_t5:\")\n",
    "for root, dirs, files in os.walk('multilingual_t5'):\n",
    "    for file in files:\n",
    "        if file.endswith('.py'):\n",
    "            filepath = os.path.join(root, file)\n",
    "            size = os.path.getsize(filepath)\n",
    "            print(f\"  {filepath} ({size} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb98c7f",
   "metadata": {},
   "source": [
    "## 5. Run Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ece9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pytest for running tests\n",
    "!pip install -q pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a26c0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try running the evaluation metrics module\n",
    "import importlib.util\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"metrics\", \"multilingual_t5/evaluation/metrics.py\")\n",
    "try:\n",
    "    metrics = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(metrics)\n",
    "    print(\"âœ“ Evaluation metrics module loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4add855",
   "metadata": {},
   "source": [
    "## 6. Key Functions Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9928e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List key modules and their contents\n",
    "import inspect\n",
    "\n",
    "modules_to_check = [\n",
    "    'multilingual_t5/tasks.py',\n",
    "    'multilingual_t5/preprocessors.py',\n",
    "    'multilingual_t5/utils.py',\n",
    "    'multilingual_t5/vocab.py'\n",
    "]\n",
    "\n",
    "for module_path in modules_to_check:\n",
    "    if os.path.exists(module_path):\n",
    "        with open(module_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            # Extract function/class definitions\n",
    "            lines = content.split('\\n')\n",
    "            funcs = [line.strip() for line in lines if line.strip().startswith('def ')]\n",
    "            classes = [line.strip() for line in lines if line.strip().startswith('class ')]\n",
    "            \n",
    "            print(f\"\\nğŸ“„ {module_path}\")\n",
    "            if classes:\n",
    "                print(f\"  Classes: {', '.join(c[:50] for c in classes[:3])}\")\n",
    "            if funcs:\n",
    "                print(f\"  Functions: {', '.join(f[:50] for f in funcs[:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd317aa",
   "metadata": {},
   "source": [
    "## 7. Example: Working with mT5\n",
    "\n",
    "This example shows how to use the mT5 model for basic text-to-text tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb93a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load and use a pre-trained mT5 model (small model for demo)\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(\"Loading mT5-Small model...\")\n",
    "model_name = \"google/mt5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "print(f\"âœ“ Model loaded: {model_name}\")\n",
    "print(f\"  Model size: {model.num_parameters():,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c86faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Translate text using mT5\n",
    "import torch\n",
    "\n",
    "def translate_text(text, source_lang=\"en\", target_lang=\"es\"):\n",
    "    \"\"\"Simple translation example\"\"\"\n",
    "    # mT5 uses task prefixes\n",
    "    input_text = f\"translate {source_lang} to {target_lang}: {text}\"\n",
    "    \n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(input_ids, max_length=50)\n",
    "    translated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return translated\n",
    "\n",
    "# Test translation\n",
    "test_text = \"Hello, how are you?\"\n",
    "result = translate_text(test_text, \"en\", \"es\")\n",
    "print(f\"English: {test_text}\")\n",
    "print(f\"Spanish: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d88f53",
   "metadata": {},
   "source": [
    "## 8. Documentation & Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbf9a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘         Multilingual T5 - Resources & Documentation        â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ğŸ“š Official Resources:\n",
    "  â€¢ GitHub: https://github.com/google-research/multilingual-t5\n",
    "  â€¢ Paper: https://arxiv.org/abs/2010.11934\n",
    "  â€¢ HuggingFace: https://huggingface.co/google/mt5-small\n",
    "\n",
    "ğŸ”§ Available Models:\n",
    "  â€¢ mt5-small (300M params)\n",
    "  â€¢ mt5-base (580M params)\n",
    "  â€¢ mt5-large (1.2B params)\n",
    "  â€¢ mt5-xl (3.7B params)\n",
    "  â€¢ mt5-xxl (13B params)\n",
    "\n",
    "ğŸ“– Languages Supported: 101 languages including\n",
    "  English, Spanish, French, German, Chinese, Arabic, Hindi, \n",
    "  and many more!\n",
    "\n",
    "ğŸ¯ Supported Tasks:\n",
    "  â€¢ Translation\n",
    "  â€¢ Question Answering\n",
    "  â€¢ Named Entity Recognition\n",
    "  â€¢ Text Classification\n",
    "  â€¢ Summarization\n",
    "\n",
    "ğŸ’¡ Next Steps:\n",
    "  1. Run the example cells above\n",
    "  2. Experiment with different inputs\n",
    "  3. Try different model sizes\n",
    "  4. Fine-tune on custom datasets\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
